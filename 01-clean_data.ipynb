{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird Species  <object>\n",
      "Distance Bin  <object>\n",
      "Date          <object>\n",
      "Time          <object>\n",
      "Coordinates   <object>\n",
      "Location      <object>\n",
      "Habitat Type  <object>\n",
      "Time Period   <object>\n",
      "Group Name    <object>\n",
      "Remarks       <object>\n"
     ]
    }
   ],
   "source": [
    "import hashlib, re, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "raw_csv = pd.read_csv('raw_data.csv')\n",
    "max_colname_len = max(\n",
    "    [len(col) for col in raw_csv.columns.values]\n",
    ")\n",
    "for col in raw_csv.columns.values:\n",
    "    print('{:{}}<{}>'.format(\n",
    "        col, max_colname_len+2, raw_csv[col].dtype)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Locations: 207\n",
      "Number of Unique Coordinates: 270\n"
     ]
    }
   ],
   "source": [
    "num_locations = len(set(raw_csv['Location']))\n",
    "num_coords = len(set(raw_csv['Coordinates']))\n",
    "print('Number of Unique Locations: {}'.format(num_locations))\n",
    "print('Number of Unique Coordinates: {}'.format(num_coords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, we need to be able to clearly distinguish survey sites.** As from the cell directly above, we cannot reliably use either the number of unique locations or coordinates in order to do this.\n",
    "\n",
    "Instead, we will use both. Going through the list from the first to last row, _adjacent_ rows with identical locations **or** coordinate values will be grouped together as one survey site. Each survey site is identifed with a hash generated from the cocatenated string of its first row's location plus coordinate and group name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_unique_sites = pd.DataFrame()\n",
    "old_row = None\n",
    "current_hash = None\n",
    "\n",
    "def get_loc_hash(row):\n",
    "    '''@returns {str} Location Hash'''\n",
    "    combined_in_bytes = (\n",
    "        row['Coordinates'] + row['Location'] + row['Group Name']\n",
    "    ).encode()\n",
    "    loc_hash = hashlib.md5(combined_in_bytes).hexdigest()\n",
    "    return loc_hash\n",
    "\n",
    "def get_new_row(row, loc_hash):\n",
    "    '''@returns {dict} Dictionary containing values for new row'''\n",
    "    new_row = dict(row)\n",
    "    new_row.pop('Date')\n",
    "    new_row['Location Hash'] = loc_hash\n",
    "    return new_row\n",
    "    '''\n",
    "    return {\n",
    "        'Bird Species': row['Bird Species'],\n",
    "        'Distance Bin': row['Distance Bin'],\n",
    "        'Time': row['Time'],\n",
    "        'Location Hash': loc_hash,\n",
    "        'Coordinates': row['Coordinates'],\n",
    "        'Location': row['Location'],\n",
    "        'Habitat': row['Habitat Type'],\n",
    "        'Time Period': row['Time Period'],\n",
    "        'Group Name': row['Group Name'],\n",
    "        'Remarks': row['Remarks']\n",
    "    }'''\n",
    "    \n",
    "def is_same_site(row1, row2):\n",
    "    if (row1 is None or row2 is None):\n",
    "        return False\n",
    "    else:\n",
    "        is_same_coords = row1['Coordinates'] == row2['Coordinates']\n",
    "        is_same_loc = row1['Location'] == row2['Location']\n",
    "        return is_same_coords or is_same_loc\n",
    "\n",
    "for _, row in raw_csv.iterrows():\n",
    "    if not is_same_site(old_row, row):\n",
    "        current_hash = get_loc_hash(row)\n",
    "    csv_unique_sites = csv_unique_sites.append(\n",
    "        get_new_row(row, current_hash),\n",
    "        ignore_index=True)\n",
    "    old_row = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = len(set(csv_unique_sites['Location Hash']))\n",
    "print('Number of Unique Location Hashes: {}'.format(num_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_unique_sites.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(csv_unique_sites['Bird Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to standardise the formatting of each column, from the right to left. **Starting with the species names, we...**\n",
    "1. Expand rows with multiple observations, e.g. Javan Myna x4\n",
    "2. Replace observations with no clear IDs with NaN values\n",
    "3. Standardise trailing white-spaces, spellings, and capitalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_names = pd.DataFrame()\n",
    "contains_digits = re.compile('\\d')\n",
    "bad_idents = re.compile(\n",
    "    '(\\`|[Un]nknown|[Ss]uspected|\\?|None|^\\-$|'\n",
    "    'sound|chweet|with| w |that|Large black bird|'\n",
    "    '[Ss]potted [Pp]igeon)'\n",
    ")\n",
    "\n",
    "def process_row(row):\n",
    "    '''@returns {tuple} <repeats: int, row: dict>'''\n",
    "    row = dict(row)  # none-destructive\n",
    "    # first, check if this row contains multiple sightings\n",
    "    if contains_digits.search(row['Bird Species']):\n",
    "        print('Has multiple: {}'.format(row['Bird Species']))\n",
    "        repeats = contains_digits.search(row['Bird Species']).group(0)\n",
    "        repeats = int(repeats)\n",
    "        row['Bird Species'] = row['Bird Species'].strip()\n",
    "        last_whitespace_i = row['Bird Species'].rfind(' ')\n",
    "        row['Bird Species'] = row['Bird Species'][:last_whitespace_i]\n",
    "    else:\n",
    "        repeats = 1\n",
    "    # then, standardise the species naming\n",
    "    row['Bird Species'] = row['Bird Species'].strip().lower().capitalize()\n",
    "    # next, check for bad IDs\n",
    "    if bad_idents.search(row['Bird Species']):\n",
    "        print('Is bad ID: {}'.format(row['Bird Species']))\n",
    "        row['Bird Species'] = np.nan\n",
    "    return (repeats, dict(row))\n",
    "        \n",
    "for _, row in csv_unique_sites.iterrows():\n",
    "    repeats, new_row = process_row(row)\n",
    "    for _ in range(repeats):\n",
    "        better_names = better_names.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(\n",
    "    [str(x) for x in list(set(better_names['Bird Species']))]\n",
    ")  # str conversion because nan is a float, not comparable with str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we standardise spellings...\n",
    "```\n",
    "Asian koele -> Asian koel\n",
    "Crow -> Jungle crow\n",
    "Javan mynah -> Javan myna\n",
    "Koel -> Asian koel\n",
    "Large-billed crow -> Jungle crow\n",
    "Olive-winged bulbol -> Olive-winged bulbul\n",
    "Olve-backed sunbird -> Olive-backed sunbird\n",
    "Pink neck green pegion -> Pink-necked green pigeon\n",
    "Pink neck green pigeon -> Pink-necked green pigeon\n",
    "Rocked pigeon -> Rock pigeon\n",
    "Swiflet -> Swiftlet\n",
    "Swiflets -> Swiftlet\n",
    "Swiftlets -> Swiftlet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "renames = {\n",
    "    'Asian koel': re.compile('(Asian koele|^Koel$)'),\n",
    "    'Jungle crow': re.compile('(^Crow$|Large-billed crow)'),\n",
    "    'Javan myna': re.compile('Javan mynah'),\n",
    "    'Olive-winged bulbul': re.compile('Olive-winged bulbol'),\n",
    "    'Olive-backed sunbird': re.compile('Olve-backed sunbird'),\n",
    "    'Pink-necked green pigeon': re.compile('Pink neck green p[ei]g[ie]on'),\n",
    "    'Rock pigeon': re.compile('Rocked pigeon'),\n",
    "    'Swiftlet': re.compile('(Swiflet[s]|Swiftlets)')\n",
    "}\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    for proper_name, has_bad_name in renames.items():\n",
    "        try:\n",
    "            if has_bad_name.search(row['Bird Species']):\n",
    "                print('Caught: {} -> {}'.format(\n",
    "                    row['Bird Species'], proper_name))\n",
    "                row['Bird Species'] = proper_name\n",
    "        except TypeError:\n",
    "            pass  # because of nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(\n",
    "    [str(x) for x in list(set(better_names['Bird Species']))]\n",
    ")  # str conversion because nan is a float, not comparable with str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Distance Bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to discount all birds in flight or heard but not seen. These observations will have a distance bin of `nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_distance = re.compile(\n",
    "    '([Hh]eard|[Ff]lyby|[Ff]light|Did not land|^\\-$)'\n",
    ")\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    try:\n",
    "        if bad_distance.search(row['Distance Bin']):\n",
    "            row['Distance Bin'] = np.nan\n",
    "    except TypeError:\n",
    "        pass  # once again, nan is a float and cannot be searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Distance Bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to standardise the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dd_space_dd = re.compile('^\\d\\d \\d\\d$')\n",
    "def fix_dd_space_dd(timestr):\n",
    "    return timestr.replace(' ', '')\n",
    "\n",
    "is_dd_colon_dd = re.compile('^\\d\\d:\\d\\d$')\n",
    "def fix_dd_colon_dd(timestr):\n",
    "    return timestr.replace(':', '')\n",
    "\n",
    "is_d_dot_d = re.compile('^\\d\\.\\d$')\n",
    "def fix_d_dot_d(timestr):\n",
    "    timestr = '0{}0'.format(timestr)\n",
    "    return timestr.replace('.', '')\n",
    "\n",
    "is_d_colon_dd = re.compile('^\\d:\\d\\d$')\n",
    "def fix_d_colon_dd(timestr):\n",
    "    timestr = '0{}'.format(timestr)\n",
    "    return fix_dd_colon_dd(timestr)\n",
    "\n",
    "is_d_dot_dd = re.compile('^\\d\\.\\d\\d$')\n",
    "def fix_d_dot_dd(timestr):\n",
    "    timestr = '0{}'.format(timestr)\n",
    "    return timestr.replace('.', '')\n",
    "\n",
    "is_ddd = re.compile('^\\d\\d\\d$')\n",
    "def fix_ddd(timestr):\n",
    "    return '0{}'.format(timestr)\n",
    "\n",
    "is_d = re.compile('^\\d$')\n",
    "def fix_d(timestr):\n",
    "    return '0{}00'.format(timestr)\n",
    "\n",
    "time_fixes = {\n",
    "    is_dd_space_dd: fix_dd_space_dd,\n",
    "    is_dd_colon_dd: fix_dd_colon_dd,\n",
    "    is_d_dot_d:     fix_d_dot_d,\n",
    "    is_d_colon_dd:  fix_d_colon_dd,\n",
    "    is_d_dot_dd:    fix_d_dot_dd,\n",
    "    is_ddd:         fix_ddd,\n",
    "    is_d:           fix_d\n",
    "}\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    for bad_time_re, fixer in time_fixes.items():\n",
    "        try:\n",
    "            if bad_time_re.search(row['Time']):\n",
    "                print('{} -> {}'.format(row['Time'], fixer(row['Time'])))\n",
    "                row['Time'] = fixer(row['Time'])\n",
    "        except TypeError:\n",
    "            pass  # ignore nan values, as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are the coordinates, which will be a bit challenging...\n",
    "\n",
    "To start, we standardise the coordinates and locations of each unique survey site (identified by it's location hash, generated earlier). \n",
    "\n",
    "This fixes typos, and the cases where a user drag-copied location and coordinate cells, causing google sheets to increment the last character if it is a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = len(set(better_names['Location']))\n",
    "num_coords = len(set(better_names['Coordinates']))\n",
    "num_unique_locations = len(set(better_names['Location Hash']))\n",
    "print('Number of Unique Locations: {}'.format(num_locations))\n",
    "print('Number of Unique Coordinates: {}'.format(num_coords))\n",
    "print('Number of Unique Location Hashes: {}'.format(num_unique_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pointer = {'hash': '', 'coord': '', 'loc': ''}\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    if row['Location Hash'] != old_pointer['hash']:\n",
    "        old_pointer['hash'] = row['Location Hash']\n",
    "        old_pointer['coord'] = row['Coordinates']\n",
    "        old_pointer['loc'] = row['Location']\n",
    "    else:\n",
    "        row['Coordinates'] = old_pointer['coord']\n",
    "        row['Location'] = old_pointer['loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = len(set(better_names['Location']))\n",
    "num_coords = len(set(better_names['Coordinates']))\n",
    "num_unique_locations = len(set(better_names['Location Hash']))\n",
    "print('Number of Unique Locations: {}'.format(num_locations))\n",
    "print('Number of Unique Coordinates: {}'.format(num_coords))\n",
    "print('Number of Unique Location Hashes: {}'.format(num_unique_locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are two duplicate coordinates, and two pairs of duplicate locations (or a triplet of identical locations). Let's figure which those are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "old_pointer = {'hash': '', 'coord': '', 'loc': ''}\n",
    "locations = []\n",
    "coords = []\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    if row['Location Hash'] != old_pointer['hash']:\n",
    "        # add each new location to list\n",
    "        locations.append(row['Location'])\n",
    "        coords.append(row['Coordinates'])\n",
    "        # reset the old_pointer\n",
    "        old_pointer['hash'] = row['Location Hash']\n",
    "        old_pointer['loc'] = row['Location']\n",
    "        old_pointer['coord'] = row['Coordinates']\n",
    "        \n",
    "location_counts = Counter(locations)\n",
    "coord_counts = Counter(coords)\n",
    "\n",
    "for loc, reps in location_counts.items():\n",
    "    if reps > 1:\n",
    "        print('Location \\'{}\\' occured {} times'.format(\n",
    "            loc, reps\n",
    "        ))\n",
    "\n",
    "for coord, reps in coord_counts.items():\n",
    "    if reps > 1:\n",
    "        print('Coordinate {} occured {} times'.format(\n",
    "            coord, reps\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat of PGP and RVRC locations are acceptable. \n",
    "\n",
    "The coordinate duplicates are for town green. These two survey sites were remarked: _\"Town Green data points were very close to one another - there might be potential recounting of bird species.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to standardise the coordinates. We need to,\n",
    "\n",
    "1. Remove verbose labellings e.g. 'Lat', 'Lon'\n",
    "2. Convert DMS notation to Decimal degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_word = re.compile('[a-zA-Z]{2,}')\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    coord = row['Coordinates']\n",
    "    coord = is_word.sub('', coord)\n",
    "    coord = coord.replace('|', ',')\n",
    "    coord = coord.replace(':', '')\n",
    "    coord = coord.replace(' ', '')\n",
    "    row['Coordinates'] = coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(better_names['Coordinates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,\n",
    "\n",
    "$$\\mathrm{D_{dec}} = \n",
    "    \\mathrm{D} + \\frac{\\mathrm{M}}{60} + \\frac{\\mathrm{S}}{3600}$$\n",
    "    \n",
    "Note that some seconds are recorded to 0 d.p. precision, others to 1 d.p.\n",
    "\n",
    "$$ 1'' = 0.0002\\bar{7}^{\\ \\circ} $$\n",
    "$$ 0.1'' = 0.00002\\bar{7}^{\\ \\circ} \\mathrm{(6\\ d.p.)}$$\n",
    "\n",
    "Therefore, we round off to 6 d.p., _if_ the decimal degrees exceeds 6 d.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dms(dms_coords):\n",
    "    digits = re.compile('\\d+\\.?\\d*')\n",
    "    return tuple(map(float, digits.findall(dms_coords)))\n",
    "\n",
    "def get_decimal(dms_xs):\n",
    "    lat_d, lat_m, lat_s = dms_xs[:3]\n",
    "    lon_d, lon_m, lon_s = dms_xs[3:]\n",
    "    lat = lat_d + lat_m/60 + lat_s/3600\n",
    "    lon = lon_d + lon_m/60 + lon_s/3600\n",
    "    lat, lon = round(lat, 6), round(lon, 6)\n",
    "    return '{},{}'.format(lat, lon)\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    if 'N' in row['Coordinates'] or 'E' in row['Coordinates']:\n",
    "        dms = get_dms(row['Coordinates'])\n",
    "        decimal = get_decimal(dms)\n",
    "        row['Coordinates'] = decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to wrap up the coordinate clean-up, we split lat and lon into two separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df = pd.DataFrame()\n",
    "\n",
    "for _, row in better_names.iterrows():\n",
    "    row = dict(row)\n",
    "    coords = row.pop('Coordinates')\n",
    "    lat, lon = coords.split(',')\n",
    "    row['Latitude'] = lat\n",
    "    row['Longitude'] = lon\n",
    "    latlon_df = latlon_df.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(latlon_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(latlon_df['Habitat Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in latlon_df.iterrows():\n",
    "    try:\n",
    "        if 'NIL' in row['Habitat Type']:\n",
    "            print('Habitat Type: {}; Distance Bin: {}'.format(\n",
    "                row['Habitat Type'], row['Distance Bin']\n",
    "            ))\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the 'Habitat Type' column. This column is largely useless, though some groups have recorded 'inflight' or 'heard but not seen' data here---while leaving legitimate values in the distance bin.\n",
    "\n",
    "Since we are using `nan` values in the distance bin to signify inflights and seen-but-not-heards, we need to do a bit of modifying. Afterwards, the habitat column can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in latlon_df.iterrows():\n",
    "    try:\n",
    "        if 'NIL' in row['Habitat Type']:\n",
    "            row['Distance Bin'] = np.nan\n",
    "    except TypeError:\n",
    "        pass  # if row['Habitat Type'] == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see the changes...\n",
    "\n",
    "for _, row in latlon_df.iterrows():\n",
    "    try:\n",
    "        if 'NIL' in row['Habitat Type']:\n",
    "            print('Habitat Type: {}; Distance Bin: {}'.format(\n",
    "                row['Habitat Type'], row['Distance Bin']\n",
    "            ))\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we inspect the remarks to see if we might need to modify any rows based on the information within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in latlon_df.iterrows():\n",
    "    # np.nan is of type float\n",
    "    if type(row['Remarks']) is not float:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection of the remarks column shows that some heard-but-not-seen and bad identifications have not had their distance bins or bird species columns respectively set to `nan`. Namely, they are:\n",
    "\n",
    "```\n",
    "Sound heard but not observed @ b39d42986a259bde849d4bd0ef74c4df  \n",
    "Heard sound but not observed, 180 degree view @ 332d8684a20391d12ae81d9d8e0aa1c9  \n",
    "Identity cannot be confirmed, but features see... @ c79289b16fa0a087f2ef63771d8dfe73\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in latlon_df.iterrows():\n",
    "    if (type(row['Remarks']) == str and (\n",
    "        (\n",
    "            'Sound heard but not observed' in row['Remarks'] or\n",
    "            'Heard sound but not observed' in row['Remarks']\n",
    "        ))):\n",
    "        print('Caught: {}'.format(row['Remarks']))\n",
    "        row['Distance Bin'] = np.nan\n",
    "    elif (type(row['Remarks']) == str and\n",
    "         'Identity cannot be confirmed' in row['Remarks']):\n",
    "        row['Bird Species'] == np.nan\n",
    "        print('Caught: {}'.format(row['Remarks']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we standardise the time period. Since all surveys are 10 minutes long, we can truncate the later bound of the time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(latlon_df['Time Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can reuse the functions from time_fixes from when\n",
    "#     we sanitised the column 'Time' earlier. Hoever,\n",
    "#     there is a new format HH.MM to include.\n",
    "time_fixes[re.compile('^\\d\\d\\.\\d\\d$')] = (\n",
    "    lambda timestr: timestr.replace('.', '')\n",
    ")\n",
    "\n",
    "for _, row in latlon_df.iterrows():\n",
    "    time_ = row['Time Period'].split('-')[0].strip()\n",
    "    # we can reuse the functions earlier from the column 'Time'\n",
    "    for bad_time_re, fixer in time_fixes.items():\n",
    "        if bad_time_re.search(time_):\n",
    "            time_ = fixer(time_)\n",
    "    row['Time Period'] = time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(latlon_df['Time Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in latlon_df.columns.values:\n",
    "    print(set(latlon_df[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save to csv, reorder the columns for presentation's sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df = latlon_df[[\n",
    "    'Bird Species',\n",
    "    'Distance Bin',\n",
    "    'Time',\n",
    "    'Location Hash',\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'Time Period',\n",
    "    'Group Name',\n",
    "    'Location',\n",
    "    'Habitat Type',\n",
    "    'Remarks'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df.to_csv('data.csv')\n",
    "print('Time taken: {:.1f}s'.format(time.time() - time_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
